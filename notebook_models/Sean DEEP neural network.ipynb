{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Extra Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The line_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext line_profiler\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "%load_ext line_profiler\n",
    "sns.set(style=\"darkgrid\")\n",
    "import requests\n",
    "import pprint\n",
    "import re\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abrea\\.julia\\packages\\Conda\\hsaaN\\deps\\usr\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3018: DtypeWarning: Columns (77,84,104,105,106,108,110,114,115,116,118,120) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time cost:  0:01:59.603237\n"
     ]
    }
   ],
   "source": [
    "time1 = datetime.now()\n",
    "filename = '../cleaned_flight_data_updated.csv'\n",
    "random.seed(234)\n",
    "p = 0.01  # p% of lines\n",
    "df = pd.read_csv(\n",
    "         filename,\n",
    "         header=0, \n",
    "         parse_dates = ['FL_DATE','dep_datetime','arr_datetime'],\n",
    "         skiprows=lambda i: i>0 and random.random() > p\n",
    ")\n",
    "time2 = datetime.now()\n",
    "print('Time cost: ', time2-time1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if DEP_DELAY is NaN, drop because useless for our model\n",
    "df = df.dropna(subset=['DEP_DELAY'])\n",
    "df['hour'] =  pd.Series.dt.hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visibility/Wind Speed Cleaning and Imputing NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time cost:  0:00:06.646230\n"
     ]
    }
   ],
   "source": [
    "# visibility column cleaning\n",
    "time1 = datetime.now()\n",
    "\n",
    "import math\n",
    "def spacetonan(x):\n",
    "    if isinstance(x, str) and x != '':\n",
    "        return float(x)\n",
    "    elif x == '':\n",
    "        return np.nan\n",
    "    return x\n",
    "\n",
    "# remove 'V', 'Vs', 's', etc.\n",
    "df['HOURLYVISIBILITY_origin'] = df['HOURLYVISIBILITY_origin'].str.replace('[^\\d(.)]', \"\")\n",
    "df['HOURLYVISIBILITY_dest'] = df['HOURLYVISIBILITY_dest'].str.replace('[^\\d(.)]', \"\")\n",
    "df['HOURLYWindSpeed_origin'] = df['HOURLYWindSpeed_origin'].str.replace('[^\\d(.)]', \"\")\n",
    "df['HOURLYWindSpeed_dest'] = df['HOURLYWindSpeed_dest'].str.replace('[^\\d(.)]', \"\")\n",
    "df['HOURLYPrecip_origin'] = df['HOURLYPrecip_origin'].str.replace('[^\\d(.)]', \"\")\n",
    "df['HOURLYPrecip_dest'] = df['HOURLYPrecip_dest'].str.replace('[^\\d(.)]', \"\")\n",
    "\n",
    "# for the entries with space, fill with nan\n",
    "df['HOURLYVISIBILITY_origin'] = list(map(spacetonan, df['HOURLYVISIBILITY_origin']))\n",
    "df['HOURLYVISIBILITY_dest'] = list(map(spacetonan, df['HOURLYVISIBILITY_dest']))\n",
    "df['HOURLYWindSpeed_origin'] = list(map(spacetonan, df['HOURLYWindSpeed_origin']))\n",
    "df['HOURLYWindSpeed_dest'] = list(map(spacetonan, df['HOURLYWindSpeed_dest']))\n",
    "df['HOURLYPrecip_origin'] = list(map(spacetonan, df['HOURLYPrecip_origin']))\n",
    "df['HOURLYPrecip_dest'] = list(map(spacetonan, df['HOURLYPrecip_dest']))\n",
    "# impute the nan values to mean (~9.4 mi visibility)\n",
    "np.set_printoptions(threshold=np.nan)\n",
    "from sklearn.impute import SimpleImputer\n",
    "imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "df['HOURLYVISIBILITY_origin_imp'] = imp_mean.fit_transform(df[['HOURLYVISIBILITY_origin']]).ravel()\n",
    "df['HOURLYVISIBILITY_dest_imp'] = imp_mean.fit_transform(df[['HOURLYVISIBILITY_dest']]).ravel()\n",
    "df['HOURLYWindSpeed_origin_imp'] = imp_mean.fit_transform(df[['HOURLYWindSpeed_origin']]).ravel()\n",
    "df['HOURLYWindSpeed_dest_imp'] = imp_mean.fit_transform(df[['HOURLYWindSpeed_dest']]).ravel()\n",
    "df['HOURLYPrecip_origin'] = imp_mean.fit_transform(df[['HOURLYPrecip_origin']]).ravel()\n",
    "df['HOURLYPrecip_dest'] = imp_mean.fit_transform(df[['HOURLYPrecip_dest']]).ravel()\n",
    "time2 = datetime.now()\n",
    "print('Time cost: ', time2-time1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing \"present weather type\" and adding column with most salient weather code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given an entry in the HOURLYPRSENTWEATHERTYPE, give AW xx value\n",
    "# otherwise return 0\n",
    "def regex_weather_cond(h_pres_w):\n",
    "    pattern = re.compile(r'\\d{2}')\n",
    "    if isinstance(h_pres_w, str):\n",
    "        if re.search(r'\\|.*?(([A-Za-z]{2,}:(\\d{2})\\s)*)\\|', h_pres_w):\n",
    "            string = re.search(r'\\|.*?(([A-Za-z]{2,}:(\\d{2})\\s)*)\\|', h_pres_w).group(0)\n",
    "            match = pattern.findall(string)\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        return 0\n",
    "    return most_severe_cond(match)\n",
    "\n",
    "# hard code severe weather condition\n",
    "# based mostly on avg delay min per weather type\n",
    "# smoke from Oct 2017 CA forestfire is high cancelled but not delay\n",
    "def most_severe_cond(lst):\n",
    "    # freezing rain\n",
    "    if '64' in lst:\n",
    "        return 64\n",
    "    elif '65' in lst:\n",
    "        return 65\n",
    "    elif '66' in lst:\n",
    "        return 66\n",
    "    # ice pellets\n",
    "    elif '74' in lst:\n",
    "        return 74\n",
    "    elif '75' in lst:\n",
    "        return 75\n",
    "    elif '76' in lst:\n",
    "        return 76\n",
    "    # thunderstorm with hailen\n",
    "    elif '93' in lst:\n",
    "        return 93\n",
    "    # snow\n",
    "    elif '71' in lst:\n",
    "        return 71\n",
    "    elif '72' in lst:\n",
    "        return 72\n",
    "    elif '73' in lst:\n",
    "        return 73\n",
    "    # fog\n",
    "    elif '30' in lst:\n",
    "        return 30\n",
    "    elif '31' in lst:\n",
    "        return 31\n",
    "    elif '32' in lst:\n",
    "        return 32\n",
    "    elif '33' in lst:\n",
    "        return 33\n",
    "    elif '34' in lst:\n",
    "        return 34\n",
    "    # safety return 0 if somehow pass empty list\n",
    "    elif len(lst) == 0:\n",
    "        return 0\n",
    "    # return max element as int from list of strings\n",
    "    # after those preceding cases, most severe is higher code\n",
    "    # also not too many cases of combination of drastically different codes \n",
    "    else:\n",
    "        return max(list(map(int, lst)))\n",
    "    \n",
    "\n",
    "# hard code severe weather condition\n",
    "# based mostly on avg delay min per weather type\n",
    "# smoke from Oct 2017 CA forestfire is high cancelled but not delay\n",
    "def ordinal_ordering(x):\n",
    "    # freezing rain\n",
    "    if x == 64:\n",
    "        return 115\n",
    "    elif x == 65:\n",
    "        return 116\n",
    "    elif x == 66:\n",
    "        return 117\n",
    "    # ice pellets\n",
    "    elif x == 74:\n",
    "        return 112\n",
    "    elif x == 75:\n",
    "        return 113\n",
    "    elif x == 76:\n",
    "        return 114\n",
    "    # thunderstorm with hail\n",
    "    elif x == 93:\n",
    "        return 113\n",
    "    # snow\n",
    "    elif x == 71:\n",
    "        return 110\n",
    "    elif x == 72:\n",
    "        return 111\n",
    "    elif x == 73:\n",
    "        return 112\n",
    "    # fog\n",
    "    elif x == 30:\n",
    "        return 105\n",
    "    elif x == 31:\n",
    "        return 106\n",
    "    elif x == 32:\n",
    "        return 107\n",
    "    elif x == 33:\n",
    "        return 108\n",
    "    elif x == 34:\n",
    "        return 109\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time cost:  0:00:01.977712\n"
     ]
    }
   ],
   "source": [
    "# make the present weather type columns into new col\n",
    "# with most severe weather code selected as integer\n",
    "time1 = datetime.now()\n",
    "df['origin_sev_code'] = df['HOURLYPRSENTWEATHERTYPE_origin'].apply(\n",
    "    lambda x: regex_weather_cond(x))\n",
    "df['dest_sev_code'] = df['HOURLYPRSENTWEATHERTYPE_dest'].apply(\n",
    "    lambda x: regex_weather_cond(x))\n",
    "\n",
    "df['origin_enc_code'] = df['origin_sev_code'].apply(\n",
    "    lambda x: ordinal_ordering(x))\n",
    "df['dest_enc_code'] = df['dest_sev_code'].apply(\n",
    "    lambda x: ordinal_ordering(x))\n",
    "time2 = datetime.now()\n",
    "print('Time cost: ', time2-time1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0 91 71 61 62  5 51 31 72 35 92 63 73 90 30 67 95 64 74 81 54 93 33 75\n",
      " 82 52 65 99  4 18 68]\n",
      "[ 0 92 71 61 31 51 91 62  5 90 72 54 30 63 74 73 35 67 95 89 18 64 52 68\n",
      " 65 75 33 93 81  4]\n",
      "[  0  91 110  61  62   5  51 106 111  35  92  63 112  90 105  67  95 115\n",
      "  81  54 113 108  82  52 116  99   4  18  68]\n",
      "[  0  92 110  61 106  51  91  62   5  90 111  54 105  63 112  35  67  95\n",
      "  89  18 115  52  68 116 113 108  81   4]\n"
     ]
    }
   ],
   "source": [
    "print(df.origin_sev_code.unique())\n",
    "print(df.dest_sev_code.unique())\n",
    "print(df.origin_enc_code.unique())\n",
    "print(df.dest_enc_code.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification\n",
    "\n",
    "Using \"DEP_DEL15\":\n",
    "- 1 means delay >= 15\n",
    "- 0 means delay < 15 (on-time / early)\n",
    "\n",
    "Goal is to predict this classification. We will use **F1-score** as the primary evaluation metric, since we are concerned with both classes of classificacation: on-time/early vs. delayed. Since the classes are unbalanced, metrics such as accuracy will give a baseline model where we classify all flights as on-time very well (which is misleading since that would not be a useful model). \n",
    "\n",
    "Let's start with a **random forest** weather model, where we predict the DEP_DEL15 using the features:\n",
    "- weather code - origin\n",
    "- visibilty - origin\n",
    "- wind speed at origin\n",
    "- precipitation at origin\n",
    "- (possibly temperature)\n",
    "\n",
    "After weather, we will use flight/airport data:\n",
    "- number of seats\n",
    "- departure airport (*need encoding/merging - use passenger count*)\n",
    "- carrier (*need ordinal encoding*)\n",
    "- departure time (DEP_TIME)\n",
    "- duration of flight\n",
    "\n",
    "Later on if we have time:\n",
    "- cloud cover (need complicated regex)\n",
    "- time dependence/delay\n",
    "- other things\n",
    "\n",
    "http://scikit-learn.org/stable/auto_examples/ensemble/plot_feature_transformation.html#sphx-glr-auto-examples-ensemble-plot-feature-transformation-py\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier\n",
    "\n",
    "https://roamanalytics.com/2016/10/28/are-categorical-variables-getting-lost-in-your-random-forests/\n",
    "\n",
    "http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/drf.html\n",
    "\n",
    "https://en.wikipedia.org/wiki/Receiver_operating_characteristic\n",
    "\n",
    "http://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "\n",
    "http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all these lol\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['YEAR',\n",
       " 'QUARTER',\n",
       " 'MONTH',\n",
       " 'DAY_OF_MONTH',\n",
       " 'DAY_OF_WEEK',\n",
       " 'FL_DATE',\n",
       " 'OP_UNIQUE_CARRIER',\n",
       " 'OP_CARRIER_AIRLINE_ID',\n",
       " 'OP_CARRIER',\n",
       " 'TAIL_NUM',\n",
       " 'OP_CARRIER_FL_NUM',\n",
       " 'ORIGIN_AIRPORT_ID',\n",
       " 'ORIGIN_AIRPORT_SEQ_ID',\n",
       " 'ORIGIN_CITY_MARKET_ID',\n",
       " 'ORIGIN',\n",
       " 'ORIGIN_CITY_NAME',\n",
       " 'ORIGIN_STATE_ABR',\n",
       " 'ORIGIN_STATE_FIPS',\n",
       " 'ORIGIN_STATE_NM',\n",
       " 'ORIGIN_WAC',\n",
       " 'DEST_AIRPORT_ID',\n",
       " 'DEST_AIRPORT_SEQ_ID',\n",
       " 'DEST_CITY_MARKET_ID',\n",
       " 'DEST',\n",
       " 'DEST_CITY_NAME',\n",
       " 'DEST_STATE_ABR',\n",
       " 'DEST_STATE_FIPS',\n",
       " 'DEST_STATE_NM',\n",
       " 'DEST_WAC',\n",
       " 'CRS_DEP_TIME',\n",
       " 'DEP_TIME',\n",
       " 'DEP_DELAY',\n",
       " 'DEP_DELAY_NEW',\n",
       " 'DEP_DEL15',\n",
       " 'DEP_DELAY_GROUP',\n",
       " 'DEP_TIME_BLK',\n",
       " 'TAXI_OUT',\n",
       " 'WHEELS_OFF',\n",
       " 'WHEELS_ON',\n",
       " 'TAXI_IN',\n",
       " 'CRS_ARR_TIME',\n",
       " 'ARR_TIME',\n",
       " 'ARR_DELAY',\n",
       " 'ARR_DELAY_NEW',\n",
       " 'ARR_DEL15',\n",
       " 'ARR_DELAY_GROUP',\n",
       " 'ARR_TIME_BLK',\n",
       " 'CANCELLED',\n",
       " 'CANCELLATION_CODE',\n",
       " 'DIVERTED',\n",
       " 'CRS_ELAPSED_TIME',\n",
       " 'ACTUAL_ELAPSED_TIME',\n",
       " 'AIR_TIME',\n",
       " 'FLIGHTS',\n",
       " 'DISTANCE',\n",
       " 'DISTANCE_GROUP',\n",
       " 'CARRIER_DELAY',\n",
       " 'WEATHER_DELAY',\n",
       " 'NAS_DELAY',\n",
       " 'SECURITY_DELAY',\n",
       " 'LATE_AIRCRAFT_DELAY',\n",
       " 'FIRST_DEP_TIME',\n",
       " 'TOTAL_ADD_GTIME',\n",
       " 'LONGEST_ADD_GTIME',\n",
       " 'DIV_AIRPORT_LANDINGS',\n",
       " 'DIV_REACHED_DEST',\n",
       " 'DIV_ACTUAL_ELAPSED_TIME',\n",
       " 'DIV_ARR_DELAY',\n",
       " 'DIV_DISTANCE',\n",
       " 'DIV1_AIRPORT',\n",
       " 'DIV1_AIRPORT_ID',\n",
       " 'DIV1_AIRPORT_SEQ_ID',\n",
       " 'DIV1_WHEELS_ON',\n",
       " 'DIV1_TOTAL_GTIME',\n",
       " 'DIV1_LONGEST_GTIME',\n",
       " 'DIV1_WHEELS_OFF',\n",
       " 'DIV1_TAIL_NUM',\n",
       " 'DIV2_AIRPORT',\n",
       " 'DIV2_AIRPORT_ID',\n",
       " 'DIV2_AIRPORT_SEQ_ID',\n",
       " 'DIV2_WHEELS_ON',\n",
       " 'DIV2_TOTAL_GTIME',\n",
       " 'DIV2_LONGEST_GTIME',\n",
       " 'DIV2_WHEELS_OFF',\n",
       " 'DIV2_TAIL_NUM',\n",
       " 'dep_time',\n",
       " 'dep_datetime',\n",
       " 'arr_time',\n",
       " 'arr_datetime',\n",
       " 'origin_WBAN',\n",
       " 'dest_WBAN',\n",
       " 'origin_weather_index',\n",
       " 'dest_weather_index',\n",
       " 'mfr_name',\n",
       " 'model',\n",
       " 'model_2',\n",
       " 'mfr_year',\n",
       " 'cert_year',\n",
       " 'eng_mfr_name',\n",
       " 'eng_model',\n",
       " 'number_of_seats',\n",
       " 'HOURLYSKYCONDITIONS_origin',\n",
       " 'HOURLYVISIBILITY_origin',\n",
       " 'HOURLYPRSENTWEATHERTYPE_origin',\n",
       " 'HOURLYDRYBULBTEMPF_origin',\n",
       " 'HOURLYRelativeHumidity_origin',\n",
       " 'HOURLYWindSpeed_origin',\n",
       " 'HOURLYWindDirection_origin',\n",
       " 'HOURLYWindGustSpeed_origin',\n",
       " 'HOURLYPrecip_origin',\n",
       " 'HOURLYAltimeterSetting_origin',\n",
       " 'HOURLYSKYCONDITIONS_dest',\n",
       " 'HOURLYVISIBILITY_dest',\n",
       " 'HOURLYPRSENTWEATHERTYPE_dest',\n",
       " 'HOURLYDRYBULBTEMPF_dest',\n",
       " 'HOURLYRelativeHumidity_dest',\n",
       " 'HOURLYWindSpeed_dest',\n",
       " 'HOURLYWindDirection_dest',\n",
       " 'HOURLYWindGustSpeed_dest',\n",
       " 'HOURLYPrecip_dest',\n",
       " 'HOURLYAltimeterSetting_dest',\n",
       " 'origin_passenger_count',\n",
       " 'dest_passenger_count',\n",
       " 'hour',\n",
       " 'HOURLYVISIBILITY_origin_imp',\n",
       " 'HOURLYVISIBILITY_dest_imp',\n",
       " 'HOURLYWindSpeed_origin_imp',\n",
       " 'HOURLYWindSpeed_dest_imp',\n",
       " 'origin_sev_code',\n",
       " 'dest_sev_code',\n",
       " 'origin_enc_code',\n",
       " 'dest_enc_code']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abrea\\.julia\\packages\\Conda\\hsaaN\\deps\\usr\\lib\\site-packages\\ipykernel_launcher.py:14: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18839  4040]\n",
      " [ 3066  1789]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.82      0.84     22879\n",
      "         1.0       0.31      0.37      0.33      4855\n",
      "\n",
      "   micro avg       0.74      0.74      0.74     27734\n",
      "   macro avg       0.58      0.60      0.59     27734\n",
      "weighted avg       0.76      0.74      0.75     27734\n",
      "\n",
      "accuracy score: 0.743780197591404\n",
      "matthews corr coef:  0.17899078084768064\n"
     ]
    }
   ],
   "source": [
    "X = df[['origin_enc_code','DEP_TIME','number_of_seats','origin_WBAN','dest_WBAN','HOURLYPrecip_origin','HOURLYPrecip_dest']]\n",
    "y = df['DEP_DEL15']\n",
    "\n",
    "#, ratio = 1.0)\n",
    "# test_size = .1,\n",
    "sm = SMOTE()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True)\n",
    "x_train_res, y_train_res = sm.fit_sample(X_train, y_train)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train_res)\n",
    "\n",
    "# Now apply the transformations to the data:\n",
    "X_train = scaler.transform(x_train_res)\n",
    "X_test = scaler.transform(X_test)\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100,20,4),max_iter=500)\n",
    "mlp.fit(X_train,y_train_res.ravel())\n",
    "predictions = mlp.predict(X_test)  \n",
    "print(confusion_matrix(y_test,predictions))\n",
    "print(classification_report(y_test,predictions))\n",
    "print(\"accuracy score:\", accuracy_score(y_test, predictions))\n",
    "print(\"matthews corr coef: \", matthews_corrcoef(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abrea\\.julia\\packages\\Conda\\hsaaN\\deps\\usr\\lib\\site-packages\\ipykernel_launcher.py:14: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  \n",
      "C:\\Users\\abrea\\.julia\\packages\\Conda\\hsaaN\\deps\\usr\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18837  4011]\n",
      " [ 3153  1733]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.82      0.84     22848\n",
      "         1.0       0.30      0.35      0.33      4886\n",
      "\n",
      "   micro avg       0.74      0.74      0.74     27734\n",
      "   macro avg       0.58      0.59      0.58     27734\n",
      "weighted avg       0.76      0.74      0.75     27734\n",
      "\n",
      "accuracy score: 0.7416889017090935\n",
      "matthews corr coef:  0.16840787241787725\n"
     ]
    }
   ],
   "source": [
    "X = df[['number_of_seats','DEP_TIME','number_of_seats','origin_WBAN','dest_WBAN','HOURLYPrecip_origin','HOURLYPrecip_dest']]\n",
    "y = df['DEP_DEL15']\n",
    "\n",
    "#, ratio = 1.0)\n",
    "# test_size = .1,\n",
    "sm = SMOTE()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True)\n",
    "x_train_res, y_train_res = sm.fit_sample(X_train, y_train)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train_res)\n",
    "\n",
    "# Now apply the transformations to the data:\n",
    "X_train = scaler.transform(x_train_res)\n",
    "X_test = scaler.transform(X_test)\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100,100,100),max_iter=100)\n",
    "mlp.fit(X_train,y_train_res.ravel())\n",
    "predictions = mlp.predict(X_test)  \n",
    "print(confusion_matrix(y_test,predictions))\n",
    "print(classification_report(y_test,predictions))\n",
    "print(\"accuracy score:\", accuracy_score(y_test, predictions))\n",
    "print(\"matthews corr coef: \", matthews_corrcoef(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abrea\\.julia\\packages\\Conda\\hsaaN\\deps\\usr\\lib\\site-packages\\ipykernel_launcher.py:13: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "X = df[['origin_enc_code','DEP_TIME','number_of_seats','origin_passenger_count','dest_passenger_count','OP_CARRIER_AIRLINE_ID'\n",
    "        ,'HOURLYPrecip_origin','DISTANCE']]\n",
    "y = df['DEP_DEL15']\n",
    "\n",
    "sm = SMOTE()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True)\n",
    "x_train_res, y_train_res = sm.fit_sample(X_train, y_train)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train_res)\n",
    "\n",
    "# Now apply the transformations to the data:\n",
    "X_train = scaler.transform(x_train_res)\n",
    "X_test = scaler.transform(X_test)\n",
    "mlp = MLPClassifier(max_iter=10)\n",
    "parameter_space = {\n",
    "    'hidden_layer_sizes': [(5000,1000,300,100,20,5)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['sgd'],\n",
    "    'alpha': [ 0.01],\n",
    "    'learning_rate': ['constant','adaptive'],\n",
    "}\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "clf = GridSearchCV(mlp, parameter_space, n_jobs=-1, cv=3)\n",
    "clf.fit(X_train, y_train_res.ravel())\n",
    "# Best paramete set\n",
    "print('Best parameters found:\\n', clf.best_params_)\n",
    "\n",
    "# All results\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n",
    "    \n",
    "predictions = clf.predict(X_test)  \n",
    "print(confusion_matrix(y_test,predictions))\n",
    "print(classification_report(y_test,predictions))\n",
    "print(\"accuracy score:\", accuracy_score(y_test, predictions))\n",
    "print(\"matthews corr coef: \", matthews_corrcoef(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abrea\\.julia\\packages\\Conda\\hsaaN\\deps\\usr\\lib\\site-packages\\ipykernel_launcher.py:13: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:\n",
      " {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (20, 3), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.660 (+/-0.016) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (20, 3), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.660 (+/-0.017) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (20, 3), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.666 (+/-0.028) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (20, 3), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.663 (+/-0.026) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (20, 3), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "[[14187  8632]\n",
      " [ 1732  3183]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.62      0.73     22819\n",
      "         1.0       0.27      0.65      0.38      4915\n",
      "\n",
      "   micro avg       0.63      0.63      0.63     27734\n",
      "   macro avg       0.58      0.63      0.56     27734\n",
      "weighted avg       0.78      0.63      0.67     27734\n",
      "\n",
      "accuracy score: 0.6263070599264441\n",
      "matthews corr coef:  0.20797780094476426\n"
     ]
    }
   ],
   "source": [
    "X = df[['origin_enc_code','DEP_TIME','number_of_seats','origin_passenger_count','dest_passenger_count','OP_CARRIER_AIRLINE_ID'\n",
    "        ,'HOURLYPrecip_origin','DISTANCE']]\n",
    "y = df['DEP_DEL15']\n",
    "\n",
    "sm = SMOTE()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True)\n",
    "x_train_res, y_train_res = sm.fit_sample(X_train, y_train)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train_res)\n",
    "\n",
    "# Now apply the transformations to the data:\n",
    "X_train = scaler.transform(x_train_res)\n",
    "X_test = scaler.transform(X_test)\n",
    "mlp = MLPClassifier(max_iter=500)\n",
    "parameter_space = {\n",
    "    'hidden_layer_sizes': [(20,3)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['sgd'],\n",
    "    'alpha': [ 0.01],\n",
    "    'learning_rate': ['constant','adaptive'],\n",
    "}\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "clf = GridSearchCV(mlp, parameter_space, n_jobs=-1, cv=3)\n",
    "clf.fit(X_train, y_train_res.ravel())\n",
    "# Best paramete set\n",
    "print('Best parameters found:\\n', clf.best_params_)\n",
    "\n",
    "# All results\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n",
    "    \n",
    "predictions = clf.predict(X_test)  \n",
    "print(confusion_matrix(y_test,predictions))\n",
    "print(classification_report(y_test,predictions))\n",
    "print(\"accuracy score:\", accuracy_score(y_test, predictions))\n",
    "print(\"matthews corr coef: \", matthews_corrcoef(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abrea\\.julia\\packages\\Conda\\hsaaN\\deps\\usr\\lib\\site-packages\\ipykernel_launcher.py:12: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  if sys.path[0] == '':\n",
      "C:\\Users\\abrea\\.julia\\packages\\Conda\\hsaaN\\deps\\usr\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:\n",
      " {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (100, 50, 10), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.640 (+/-0.020) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.716 (+/-0.098) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.639 (+/-0.024) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.709 (+/-0.093) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.643 (+/-0.018) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (100, 50, 10), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.707 (+/-0.091) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (100, 50, 10), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.642 (+/-0.021) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (100, 50, 10), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.710 (+/-0.085) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (100, 50, 10), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.630 (+/-0.016) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': 100, 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.645 (+/-0.021) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': 100, 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.627 (+/-0.015) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': 100, 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.646 (+/-0.021) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': 100, 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.641 (+/-0.019) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (100, 20, 4), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.698 (+/-0.085) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (100, 20, 4), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.640 (+/-0.019) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (100, 20, 4), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.676 (+/-0.060) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (100, 20, 4), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.631 (+/-0.019) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (20, 5), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.646 (+/-0.026) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (20, 5), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.633 (+/-0.023) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (20, 5), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.646 (+/-0.019) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (20, 5), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.637 (+/-0.026) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.639 (+/-0.033) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.636 (+/-0.023) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.642 (+/-0.025) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.641 (+/-0.019) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (100, 50, 10), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.643 (+/-0.020) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (100, 50, 10), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.642 (+/-0.019) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (100, 50, 10), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.645 (+/-0.018) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (100, 50, 10), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.625 (+/-0.011) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': 100, 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.642 (+/-0.016) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': 100, 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.626 (+/-0.012) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': 100, 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.642 (+/-0.019) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': 100, 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.638 (+/-0.022) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (100, 20, 4), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.645 (+/-0.023) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (100, 20, 4), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.638 (+/-0.019) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (100, 20, 4), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.644 (+/-0.019) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (100, 20, 4), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.633 (+/-0.023) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (20, 5), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.644 (+/-0.020) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (20, 5), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.631 (+/-0.018) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (20, 5), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.645 (+/-0.021) for {'activation': 'tanh', 'alpha': 0.1, 'hidden_layer_sizes': (20, 5), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.649 (+/-0.023) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.724 (+/-0.113) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.649 (+/-0.025) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.718 (+/-0.088) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.652 (+/-0.021) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (100, 50, 10), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.732 (+/-0.118) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (100, 50, 10), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.651 (+/-0.022) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (100, 50, 10), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.734 (+/-0.096) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (100, 50, 10), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.635 (+/-0.021) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': 100, 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.660 (+/-0.032) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': 100, 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.635 (+/-0.021) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': 100, 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.667 (+/-0.037) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': 100, 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.647 (+/-0.019) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (100, 20, 4), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.718 (+/-0.086) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (100, 20, 4), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.649 (+/-0.021) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (100, 20, 4), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.710 (+/-0.072) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (100, 20, 4), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.640 (+/-0.017) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (20, 5), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.646 (+/-0.015) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (20, 5), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.634 (+/-0.023) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (20, 5), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.648 (+/-0.021) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (20, 5), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.648 (+/-0.021) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.677 (+/-0.032) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.646 (+/-0.019) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.683 (+/-0.056) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.648 (+/-0.019) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (100, 50, 10), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.666 (+/-0.033) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (100, 50, 10), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.649 (+/-0.022) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (100, 50, 10), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.671 (+/-0.024) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (100, 50, 10), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.634 (+/-0.021) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': 100, 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.648 (+/-0.019) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': 100, 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.634 (+/-0.020) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': 100, 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.649 (+/-0.017) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': 100, 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.646 (+/-0.027) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (100, 20, 4), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.675 (+/-0.060) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (100, 20, 4), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.643 (+/-0.027) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (100, 20, 4), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.663 (+/-0.041) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (100, 20, 4), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.636 (+/-0.026) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (20, 5), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.647 (+/-0.019) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (20, 5), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.636 (+/-0.021) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (20, 5), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.647 (+/-0.020) for {'activation': 'relu', 'alpha': 0.1, 'hidden_layer_sizes': (20, 5), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "[[18558  4182]\n",
      " [ 3053  1941]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.82      0.84     22740\n",
      "         1.0       0.32      0.39      0.35      4994\n",
      "\n",
      "   micro avg       0.74      0.74      0.74     27734\n",
      "   macro avg       0.59      0.60      0.59     27734\n",
      "weighted avg       0.76      0.74      0.75     27734\n",
      "\n",
      "accuracy score: 0.739128867094541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matthews corr coef:  0.18969155618293748\n"
     ]
    }
   ],
   "source": [
    "X = df[['origin_enc_code','DEP_TIME','number_of_seats','origin_WBAN','dest_WBAN','HOURLYPrecip_origin','HOURLYPrecip_dest']]\n",
    "y = df['DEP_DEL15']\n",
    "\n",
    "sm = SMOTE()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True)\n",
    "x_train_res, y_train_res = sm.fit_sample(X_train, y_train)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train_res)\n",
    "\n",
    "# Now apply the transformations to the data:\n",
    "X_train = scaler.transform(x_train_res)\n",
    "X_test = scaler.transform(X_test)\n",
    "mlp = MLPClassifier(max_iter=100)\n",
    "parameter_space = {\n",
    "    'hidden_layer_sizes': [ (50,50,50), (100,50,50), (100), (100,20,4), (20,5)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'alpha': [ 0.01, 0.1],\n",
    "    'learning_rate': ['constant','adaptive'],\n",
    "}\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "clf = GridSearchCV(mlp, parameter_space, n_jobs=-1, cv=3)\n",
    "clf.fit(X_train, y_train_res.ravel())\n",
    "# Best paramete set\n",
    "print('Best parameters found:\\n', clf.best_params_)\n",
    "\n",
    "# All results\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n",
    "    \n",
    "predictions = clf.predict(X_test)  \n",
    "print(confusion_matrix(y_test,predictions))\n",
    "print(classification_report(y_test,predictions))\n",
    "print(\"accuracy score:\", accuracy_score(y_test, predictions))\n",
    "print(\"matthews corr coef: \", matthews_corrcoef(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abrea\\.julia\\packages\\Conda\\hsaaN\\deps\\usr\\lib\\site-packages\\ipykernel_launcher.py:12: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[15804  7011]\n",
      " [ 2112  2807]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.69      0.78     22815\n",
      "         1.0       0.29      0.57      0.38      4919\n",
      "\n",
      "   micro avg       0.67      0.67      0.67     27734\n",
      "   macro avg       0.58      0.63      0.58     27734\n",
      "weighted avg       0.78      0.67      0.71     27734\n",
      "\n",
      "matthews corr coef:  0.2103508825488134\n",
      "roc auc score: 0.6316733047760305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abrea\\.julia\\packages\\Conda\\hsaaN\\deps\\usr\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "X = df[['origin_enc_code','DEP_TIME','number_of_seats','origin_passenger_count','dest_passenger_count','OP_CARRIER_AIRLINE_ID'\n",
    "        ,'HOURLYPrecip_origin','DISTANCE']]\n",
    "y = df['DEP_DEL15']\n",
    "sm = SMOTE()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True)\n",
    "x_train_res, y_train_res = sm.fit_sample(X_train, y_train)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train_res)\n",
    "\n",
    "# Now apply the transformations to the data:\n",
    "X_train = scaler.transform(x_train_res)\n",
    "X_test = scaler.transform(X_test)\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(40,20,10,5,5,2),max_iter=100)\n",
    "mlp.fit(X_train,y_train_res.ravel())\n",
    "predictions = mlp.predict(X_test)  \n",
    "print(confusion_matrix(y_test,predictions))\n",
    "print(classification_report(y_test,predictions))\n",
    "print(\"matthews corr coef: \", matthews_corrcoef(y_test, predictions))\n",
    "print(\"roc auc score:\", roc_auc_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
